{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "351f9f1d",
   "metadata": {},
   "source": [
    "## Principal Component Analysis\n",
    "\n",
    "The principal application of PCA is **dimension reduction**. If we have high dimensional data, PCA allows us to reduce the dimensionality of the data so the bulk of the variation that exists in your data across many high dimensions is captured in fewer dimensions. \n",
    "\n",
    "The key idea for PCA is to help reducing dimension of the dataset to preserve the most information (variation) of the dataset.\n",
    "\n",
    "### Curse of Dimensionality\n",
    "\n",
    "**Sparseness in N-Dimensional Space:** \n",
    "\n",
    "Curse of dimensionality refers to a set of problems that arise when working with high-dimensional data. The dimension of a dataset corresponds to the number of attributes/features that exist in a dataset. The popular aspects of the curse of dimensionality includes \"data sparsity\" and \"distance concentration\".\n",
    "\n",
    "**Data Sparsity**:\n",
    "When the training samples do not capture all combinations of the attributes, is referred to as \"data sparsity\" or simply \"sparsity\" in high dimensional data. Training a model with sparse data could lead to high-variance or overfitting condition because the model has learned from the frequently occurring combinations of the attributes and can predict the outcome accurately, but failed to predict less frequently occurring combinations in the reality (unseen data).\n",
    "\n",
    "**Distance Concentration**:\n",
    "Distance concentration refers to the problem of all the pairwise distances between different samples/points in the space converging to the same value as the dimensionality of the data increases. Several machine learning models such as clustering or nearest neighboursâ€™ methods use distance-based metrics to identify similar or proximity of the samples. Due to distance concentration, the concept of proximity or similarity of the samples may not be qualitatively relevant in higher dimensions.\n",
    "\n",
    "\n",
    "### Steps to Conducting PCA\n",
    "\n",
    "#### Mathematical Approach:\n",
    "\n",
    "1. Center each feature by subtracting the feature mean\n",
    "2. Calculate the covariance matrix for your normalized dataset (Eigen Decomposition)\n",
    "3. Calculate the eigenvectors/eigenvalues for the covariance matrix\n",
    "4. Take the dot product of the transpose of the eigenvectors with the transpose of the normalized data\n",
    "\n",
    "#### Graphical Approach:\n",
    "\n",
    "Assume two predictors / features case: $X_1$ and $X_2$\n",
    "\n",
    "1. Plot the two predictor on the graph with scatter plot\n",
    "\n",
    "![PCA1](img/pca1.png)\n",
    "\n",
    "2. Standard scale both $X_1$ and $X_2$ and fit the best PC line to the data\n",
    "\n",
    "![PCA2](img/pca2.png)\n",
    "\n",
    "3. Maximize the Sum of Squared Distance (SSD) for PC1\n",
    "\n",
    "![PCA3](img/pca3.png)\n",
    "\n",
    "Note: SSD for PC1 is the Eigenvalue for PC1\n",
    "\n",
    "4. Calculate the Eigenvector for PC1\n",
    "\n",
    "![PCA4](img/pca4.png)\n",
    "\n",
    "Note: Eigenvector for PC1 is the called \"singular vector\", which consists of $\\frac{\\Delta X_1}{m}$ and $\\frac{\\Delta X_2}{m}$, where $m$ is the slope of PC1.\n",
    "\n",
    "5. PC2 is just a perpendicular line of PC1 in this case\n",
    "\n",
    "![PCA5](img/pca5.png)\n",
    "\n",
    "Note: the Eigenvalue and Eigenvector can be calculated using the same logic to the PC2 line.\n",
    "\n",
    "6. Rotate the plot to make PC1 the horizontal axis and PC2 the vertical axis\n",
    "\n",
    "![PCA6](img/pca6.png)\n",
    "\n",
    "At this stage, we can calculate the variation for PC1 and PC2:\n",
    "\n",
    "$Variation_{PC1} = \\frac{SSD_{PC1}}{n-1}$\n",
    "\n",
    "$Variation_{PC2} = \\frac{SSD_{PC2}}{n-1}$\n",
    "\n",
    "Total Variation = $Variation_{PC1} + Variation_{PC2}$\n",
    "\n",
    "Therefore, the total variation of the data being explained by PC1 and PC 2:\n",
    "\n",
    "$$Explained_{PC1} = \\frac{Variation_{PC1}}{TotalVariation}$$\n",
    "\n",
    "$$Explained_{PC2} = \\frac{Variation_{PC2}}{TotalVariation}$$\n",
    "\n",
    "For higher dimensional data, the Eigenvector for each principal component consists the total number of elements that matches with the total numbers of the predictors / features in the dataset.\n",
    "\n",
    "e.g. Principal Component Functions\n",
    "\n",
    "$$PC_1 = ev_{11}(x_1) + ev_{12}(x_2) + ... + ev_{1p}(x_p)$$\n",
    "\n",
    "$$PC_2 = ev_{21}(x_1) + ev_{22}(x_2) + ... + ev_{2p}(x_p)$$\n",
    "\n",
    "$$PC_3 = ev_{31}(x_1) + ev_{32}(x_2) + ... + ev_{3p}(x_p)$$\n",
    "\n",
    "...\n",
    "\n",
    "where \n",
    "\n",
    "- $ev_{11}, ev_{12}, ... ev_{1p}$ are the values in the eigenvector (which is calculated by $\\frac{\\Delta X_1}{m}, \\frac{\\Delta X_2}{m}$, ... in our example)   \n",
    "- $x_1, x_2, ... x_p$ are the predictors columns  \n",
    "- $p$ refers to the number of predictors in the dataset  \n",
    "\n",
    "### Python Code\n",
    "\n",
    "SKLearn Documentation: https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html\n",
    "\n",
    "**Calculate the Explained Variance by the PCA**\n",
    "\n",
    "``` Python\n",
    "# Import dependencies\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Standard Scale the predictors\n",
    "X = StandardScaler().fit_transform(X)\n",
    "\n",
    "# Create a PCA object that reduce dimensions into 2 principal components\n",
    "pca = PCA(n_components = 2)\n",
    "\n",
    "# Fit the predictors to the PCA object\n",
    "principalComponents = pca.fit_transform(X)\n",
    "\n",
    "# Putting the components into a dataframe\n",
    "principal_df = pd.DataFrame(data = principalComponents,\n",
    "                            columns = ['principal component 1', 'principal component 2'])\n",
    "\n",
    "# Explained variance by the PCA\n",
    "pca.explained_variance_ratio_\n",
    "```\n",
    "\n",
    "#### Diagnostic of the Principal Components\n",
    "\n",
    "We usually use the **scree plot** to diagnose the principal components to determine the number of components to include in the model.\n",
    "\n",
    "``` Python\n",
    "# Import dependencies\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create the PC values for the horizontal axis\n",
    "PCs = ['PC1', 'PC2', 'PC3', ....]\n",
    "\n",
    "# Create the explained variance values for each component\n",
    "exp_var = pca.explained_variance_ratio_.tolist()\n",
    "\n",
    "# Create bar plot with the explained variance ratio\n",
    "plt.bar(PCs, exp_var, color='blue')\n",
    "plt.title('Scree Plot')\n",
    "plt.xlabel('Principal Components')\n",
    "plt.ylabel('Proportion of Variance Explained')\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "**Dimension Reduction by Minimum Explained Variance Threshold**\n",
    "\n",
    "``` Python\n",
    "# Import dependencies\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Standard Scale the predictors\n",
    "X = StandardScaler().fit_transform(X)\n",
    "\n",
    "# Create a PCA object that choose the minimum number of principal component such that 90% of the variance is retained\n",
    "pca_90 = PCA(n_components = 0.90)\n",
    "\n",
    "# Fit PCA on training set\n",
    "pca_90.fit(X_train)\n",
    "\n",
    "# Transform both training set and test set with the pca_90 object\n",
    "pca_X_train = pca.transform(X_train)\n",
    "pca_X_Test = pca.transform(X_test)\n",
    "\n",
    "# Apply transformed training set for ML model, like logistic regression\n",
    "lr = LogisticRegression(solver='lbfgs')\n",
    "lr.fit(pca_X_train, y_train)\n",
    "\n",
    "# Make prediction based on the testing set\n",
    "lr.predict(pca_X_test)\n",
    "\n",
    "# Calculate the evaluation metric (consider using other measure based on situation)\n",
    "lr.score(pca_X_test, y_test)\n",
    "```\n",
    "\n",
    "You can try setting different ```n_components``` values, such as 0.95, 0.9, 0.85, 0.8, 0.75, 0.7 ... to compare the number of components (features or predictors) applied to the model, time for training, and testing accuracy rate to see if it makes a significant improvement on training time without too much of accuracy rate.\n",
    "\n",
    "\n",
    "\n",
    "Now, lets play around with this image dataset from SKLearn and see if you can reduce the number of features for accuracy prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e435f1a0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-16T19:59:22.382719Z",
     "start_time": "2021-09-16T19:59:21.967872Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import dependencies\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.datasets import fetch_openml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d3c89c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-16T19:48:33.935116Z",
     "start_time": "2021-09-16T19:48:00.821127Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import the image dataset from SKLearn\n",
    "mnist = fetch_openml('mnist_784')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf26ba5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-16T19:48:34.374727Z",
     "start_time": "2021-09-16T19:48:33.938124Z"
    }
   },
   "outputs": [],
   "source": [
    "# Test Train Split\n",
    "train_img, test_img, train_lbl, test_lbl = train_test_split(\n",
    "    mnist.data, mnist.target, test_size=0.15, random_state=999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c553de33",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-16T19:48:35.754372Z",
     "start_time": "2021-09-16T19:48:34.376849Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create the Scaling object\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit on training set only.\n",
    "scaler.fit(train_img)\n",
    "\n",
    "# Apply transform to both the training set and the test set.\n",
    "train_img = scaler.transform(train_img)\n",
    "test_img = scaler.transform(test_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9372b204",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-16T19:58:54.978931Z",
     "start_time": "2021-09-16T19:58:45.172919Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create a PCA object with 500 principle components\n",
    "\n",
    "\n",
    "# Fit the training data to the PCA\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d835b5a4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-16T20:10:10.364854Z",
     "start_time": "2021-09-16T20:10:09.618865Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create the PC values for the horizontal axis\n",
    "\n",
    "\n",
    "# Create the explained variance for each component\n",
    "\n",
    "\n",
    "# Create bar plot with the explained variance ratio\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f1fc8f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-16T19:48:42.535565Z",
     "start_time": "2021-09-16T19:48:35.767009Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create the PCA object to preserve minimum 90% of variance \n",
    "\n",
    "\n",
    "# Fit on training set only\n",
    "\n",
    "\n",
    "# Transform the training set and testing set with the PCA object\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6b1e27",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-16T19:49:04.372091Z",
     "start_time": "2021-09-16T19:49:04.362922Z"
    }
   },
   "outputs": [],
   "source": [
    "# Build your own model for this classification problem\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e38b68bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate the prediction with CV or testing set\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0391c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to create a for loop to run the process with different n_component values and track the accuracy and run time\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0836125",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29e3268",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
